{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dropout, Dense\n",
        "from tensorflow.keras.layers import Flatten, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "4y3NfDF4GxjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install pickle5\n",
        "# import pickle5 as pickle"
      ],
      "metadata": {
        "id": "jHOGyC0iGz7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAXLEN = 300"
      ],
      "metadata": {
        "id": "BirGlI3vG5Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create train and test sets\n",
        "def load_data():\n",
        "    arxiv = pd.read_csv('./Data/SampledArxiv.csv')\n",
        "\n",
        "    labels = arxiv[arxiv.columns[1:]]\n",
        "\n",
        "    X = []\n",
        "    sentences = list(arxiv[\"Text\"])\n",
        "    for sen in sentences:\n",
        "        X.append(sen)\n",
        "    y = labels.values\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "xQEz-_KkKLkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for tokenizing text\n",
        "def get_tokens(X_train, X_test, tokenizer_path=None, save_tokenizer=False):\n",
        "    if not tokenizer_path:\n",
        "      tokenizer = Tokenizer(num_words=15000)\n",
        "      tokenizer.fit_on_texts(X_train)\n",
        "      if save_tokenizer:\n",
        "        file = open('./Utils/tokenizer.pkl', 'wb')\n",
        "        pickle.dump(tokenizer, file)\n",
        "        file.close()\n",
        "    else:\n",
        "      tokenizer = pickle.load(open(tokenizer_path, 'rb'))\n",
        "    \n",
        "    X_train = tokenizer.texts_to_sequences(X_train)\n",
        "    X_test = tokenizer.texts_to_sequences(X_test)\n",
        "    X_train = pad_sequences(X_train, padding='post', maxlen=MAXLEN)\n",
        "    X_test = pad_sequences(X_test, padding='post', maxlen=MAXLEN)\n",
        "\n",
        "    return X_train, X_test, tokenizer"
      ],
      "metadata": {
        "id": "zgd8xBzHKPW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for generating emebedding matrix using GloVe embeddings\n",
        "def get_embeddings(tokenizer, embedding_matrix_path=None, save_matrix=False):\n",
        "    if not embedding_matrix_path:\n",
        "      embeddings_dictionary = dict()\n",
        "      vocab_size = len(tokenizer.word_index) + 1\n",
        "      glove_file = open('./glove.840B.300d.txt', encoding=\"utf8\")\n",
        "\n",
        "      for line in glove_file:\n",
        "        records = line.split(' ')\n",
        "        word = records[0]\n",
        "        vector_dimensions = np.asarray(records[1:])\n",
        "        embeddings_dictionary[word] = vector_dimensions\n",
        "      glove_file.close()\n",
        "\n",
        "      embedding_matrix = np.zeros((vocab_size, 300))\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        embedding_vector = embeddings_dictionary.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "      \n",
        "      if save_matrix:\n",
        "        file = open('./Utils/arxiv_embedding_matrix.pkl', 'wb')\n",
        "        pickle.dump(embedding_matrix, file)\n",
        "        file.close()\n",
        "    else:\n",
        "      embedding_matrix = pickle.load(open('./Utils/arxiv_embedding_matrix.pkl', 'rb'))\n",
        "    \n",
        "    return embedding_matrix"
      ],
      "metadata": {
        "id": "q8NjF5SgKjS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for preprocessing text for inference\n",
        "def pre_process(text, tokenizer):\n",
        "    text = tokenizer.texts_to_sequences([text])\n",
        "    text = pad_sequences(text, padding='post', maxlen=MAXLEN)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "zehz1nnaKT48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PoaiVAs53vM"
      },
      "outputs": [],
      "source": [
        "#Function for building model(either using GloVe embeddings or without)\n",
        "def build_model(tokenizer, use_glove=True):\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    deep_inputs = Input(shape=(MAXLEN,))\n",
        "    \n",
        "    if use_glove:\n",
        "      embedding_matrix = get_embeddings(tokenizer, embedding_matrix_path=None, save_matrix=True)  # If matrix is already saved, set embedding_matrix_path to file path\n",
        "      embedding_layer = Embedding(vocab_size, 300, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
        "    else:\n",
        "      embedding_layer = Embedding(vocab_size, 300)(deep_inputs)\n",
        "    \n",
        "    lstm1 = LSTM(128)(embedding_layer)\n",
        "    drop = Dropout(0.3)(lstm1)\n",
        "    dense_layer_1 = Dense(11, activation='sigmoid')(drop)\n",
        "    model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.config.list_physical_devices('GPU'))\n",
        "X_train, X_test, y_train, y_test = load_data()\n",
        "X_train, X_test, tokenizer = get_tokens(X_train, X_test, tokenizer_path=None, save_tokenizer=True)  # If tokenizer is already saved, set tokenizer_path to file path\n",
        "model = build_model(tokenizer=tokenizer, use_glove=True)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc',\n",
        "                                                                      tfa.metrics.F1Score(num_classes=11,average='micro', name='F1_micro'),\n",
        "                                                                      tfa.metrics.F1Score(num_classes=11,average='macro', name='F1_macro')])\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=3, verbose=1, validation_split=0.2)\n",
        "model.save('./Models/lstm_Glove')\n",
        "\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])\n",
        "print(\"Test F1 micro:\", score[2])\n",
        "print(\"Test F1 macro:\", score[3])"
      ],
      "metadata": {
        "id": "NnhyudsNSFp9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}